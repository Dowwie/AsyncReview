# Runbook Report: mem0
Repository: /Users/dgordon/projects/mem0
Date: 2026-02-23 13:29
Questions answered: 9/16

## Q1: What is this project? What problem does it solve and how is the codebase organized at a high level?

# Mem0: The Memory Layer for Personalized AI

## What is this project?

Mem0 (pronounced "mem-zero") is an open-source memory layer for AI assistants and agents. It provides intelligent, persistent memory capabilities that enable AI applications to remember user preferences, adapt to individual needs, and continuously learn over time. The project is developed by Mem0.ai (Y Combinator S24) and is available as both a self-hosted open-source package and a hosted platform service.

## What problem does it solve?

AI assistants and chatbots traditionally suffer from a fundamental limitation: they don't remember interactions across sessions. Each conversation starts from scratch, forcing users to repeat information and preventing truly personalized experiences.

Mem0 solves this by providing:

1. **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
2. **Persistent Storage**: Memories are stored durably using vector databases and can be retrieved semantically
3. **Intelligent Retrieval**: Uses embeddings and vector search to find relevant memories based on context
4. **Continuous Learning**: Automatically extracts and stores new memories from conversations
5. **Cross-Platform Support**: Works with multiple LLMs (OpenAI, Anthropic, Azure, Gemini, etc.) and vector stores (Pinecone, Qdrant, Chroma, Milvus, etc.)

**Key Benefits** (from their research):
- +26% Accuracy over OpenAI Memory on the LOCOMO benchmark
- 91% Faster responses than full-context approaches
- 90% Lower token usage

**Use Cases**: AI Assistants, Customer Support, Healthcare, Productivity tools, Gaming - any application where remembering user context improves the experience.

## Codebase Organization

The codebase contains 193 files organized as follows:

```
mem0/                    # Main package (142 files)
├── client/              # API client for hosted Mem0 platform
│   ├── main.py          # MemoryClient class for cloud API
│   ├── project.py       # Project management (sync/async)
│   └── utils.py         # Client utilities
├── configs/             # Configuration classes (48 files)
│   ├── embeddings/      # Embedding model configs
│   ├── llms/            # LLM provider configs (OpenAI, Anthropic, Azure, DeepSeek, etc.)
│   ├── rerankers/       # Reranker configs (Cohere, HuggingFace, LLM-based)
│   ├── vector_stores/   # Vector store configs (Pinecone, Qdrant, Chroma, etc.)
│   ├── base.py          # Base MemoryConfig
│   ├── enums.py         # MemoryType and other enums
│   └── prompts.py       # System prompts for memory operations
├── embeddings/          # Embedding model implementations (15 providers)
├── graphs/              # Graph memory support (Neptune integration)
├── llms/                # LLM integrations (20 files, 15+ providers)
├── memory/              # Core memory implementation
│   ├── main.py          # Main Memory class with add/search/get/delete
│   ├── base.py          # MemoryBase abstract class
│   ├── graph_memory.py  # Graph-based memory
│   ├── storage.py       # SQLiteManager for local storage
│   └── utils.py         # Memory processing utilities
├── proxy/               # Proxy utilities
├── reranker/            # Reranking implementations
├── utils/               # Factory classes
│   └── factory.py       # LlmFactory, EmbedderFactory, VectorStoreFactory, etc.
└── vector_stores/       # Vector database integrations (26 files, 20+ stores)

tests/                   # Test suite (49 files)
├── configs/             # Configuration tests
├── embeddings/          # Embedding tests
├── llms/                # LLM tests
└── ...

README.md                # Project documentation
```

**Architecture Highlights**:
- **Factory Pattern**: `utils/factory.py` provides factories for creating LLM, embedding, vector store, and reranker instances
- **Abstract Base Class**: `memory/base.py` defines the MemoryBase interface (get, get_all, update, delete, history)
- **Config-Driven**: Each component has dedicated config classes in `configs/`
- **Extensible**: Easy to add new LLM providers, embedding models, or vector stores by following existing patterns


## Q2: How does this project handle pluggable backends or providers? What extension points exist for databases, vector stores, or storage?


# Mem0 Pluggable Backend Architecture

Mem0 implements a robust plugin architecture for databases, vector stores, LLMs, embeddings, and storage providers using three key patterns: Abstract Base Classes, Factory Pattern, and Configuration-based Registration.

## 1. Abstract Base Classes (Extension Contracts)

### VectorStoreBase (mem0/vector_stores/base.py)
The core interface for vector store implementations with 12 abstract methods:
- create_col, insert, search, delete, update, get, list_cols, delete_col, col_info, list, reset

**Supported Vector Stores (23+):** Qdrant, Chroma, PGVector, Milvus, Pinecone, MongoDB, Redis, Elasticsearch, Weaviate, FAISS, Supabase, OpenSearch, Vertex AI, Azure AI Search, Databricks, Cassandra, Neptune, and more.

### EmbeddingBase (mem0/embeddings/base.py)
Single abstract method: embed(text, memory_action)

**Supported Embedding Providers (14+):** OpenAI, Ollama, Azure OpenAI, Gemini, VertexAI, HuggingFace, Together, LMStudio, FastEmbed, AWS Bedrock, Langchain.

### LLMBase (mem0/llms/base.py)
Abstract method: generate_response(messages, tools, tool_choice, **kwargs)

**Supported LLM Providers (17+):** OpenAI, Anthropic, Azure OpenAI, Gemini, Groq, Together, Ollama, DeepSeek, XAI, LiteLLM, AWS Bedrock, LMStudio, VLLM, Langchain.

### BaseReranker (mem0/reranker/base.py)
Abstract method: rerank(query, documents, top_k)

**Supported Rerankers (5):** Cohere, Sentence Transformer, HuggingFace, Zero Entropy, LLM Reranker.

## 2. Factory Pattern (Dynamic Instantiation)

Located in mem0/utils/factory.py:

- **LlmFactory** - 17+ providers with register_provider() method for dynamic registration
- **EmbedderFactory** - 14+ providers with register_provider() method
- **VectorStoreFactory** - 23+ providers (no register_provider, manual registration)
- **GraphStoreFactory** - 5 providers (memgraph, neptune, neptunedb, kuzu, default)
- **RerankerFactory** - 5 providers with register_provider() method

Each factory has a create() classmethod that instantiates the appropriate provider based on configuration.

## 3. Configuration System

### MemoryConfig (mem0/configs/base.py)
Central Pydantic configuration tying all components:
- vector_store: VectorStoreConfig
- llm: LlmConfig  
- embedder: EmbedderConfig
- graph_store: GraphStoreConfig
- reranker: RerankerConfig

Each provider has dedicated Pydantic config classes with validation (e.g., QdrantConfig, ChromaDbConfig, OpenAIConfig).

## 4. Storage Layer

SQLiteManager (mem0/memory/storage.py) - Manages memory history with SQLite database.

## 5. How to Add a New Provider

1. Implement the appropriate base class (e.g., VectorStoreBase)
2. Create a Pydantic configuration class
3. Register with the factory:
   - LlmFactory.register_provider("name", "module.ClassPath", ConfigClass)
   - Or manually add to provider_to_class dictionary

## 6. Summary of Extension Points

| Component | Base Class | Factory | Providers | Dynamic Registration |
|-----------|-----------|---------|-----------|---------------------|
| Vector Store | VectorStoreBase | VectorStoreFactory | 23+ | Manual |
| LLM | LLMBase | LlmFactory | 17+ | Yes (register_provider) |
| Embeddings | EmbeddingBase | EmbedderFactory | 14+ | Yes (register_provider) |
| Reranker | BaseReranker | RerankerFactory | 5 | Yes (register_provider) |
| Graph Store | MemoryBase | GraphStoreFactory | 5 | Manual |


## Q3: What interface or base abstraction do vector store backends implement? What are the method signatures and return types?

# Vector Store Backend Interface

## Base Abstraction

The vector store backends implement the `VectorStoreBase` abstract base class located in `mem0/vector_stores/base.py`. It uses Python's `abc` module to define an abstract interface.

python
from abc import ABC, abstractmethod

class VectorStoreBase(ABC):
``n
## Abstract Method Signatures

The `VectorStoreBase` interface defines 11 abstract methods:

| Method | Signature | Description |
|--------|-----------|-------------|
| `create_col` | `create_col(self, name, vector_size, distance)` | Create a new collection |
| `insert` | `insert(self, vectors, payloads=None, ids=None)` | Insert vectors into a collection |
| `search` | `search(self, query, vectors, limit=5, filters=None)` | Search for similar vectors |
| `delete` | `delete(self, vector_id)` | Delete a vector by ID |
| `update` | `update(self, vector_id, vector=None, payload=None)` | Update a vector and its payload |
| `get` | `get(self, vector_id)` | Retrieve a vector by ID |
| `list_cols` | `list_cols(self)` | List all collections |
| `delete_col` | `delete_col(self)` | Delete a collection |
| `col_info` | `col_info(self)` | Get information about a collection |
| `list` | `list(self, filters=None, limit=None)` | List all memories |
| `reset` | `reset(self)` | Reset by deleting the collection and recreating it |

## Return Types

The base class does not define return type annotations, but concrete implementations follow consistent patterns:

### OutputData Model

Most implementations define a `OutputData` Pydantic model for structured return values:

python
from pydantic import BaseModel
from typing import Optional, Dict

class OutputData(BaseModel):
    id: Optional[str]      # memory id
    score: Optional[float] # distance/similarity score
    payload: Optional[Dict] # metadata dictionary
``n
### Typical Return Types by Method

| Method | Return Type |
|--------|-------------|
| `search` | `List[OutputData]` (list of OutputData objects) |
| `get` | `OutputData` (single OutputData object) or `dict` |
| `list` | `List[OutputData]` (list of OutputData objects) |
| `list_cols` | `List[Collection]` (list of collection objects, type varies by backend) |
| `col_info` | `Dict` (dictionary with collection metadata) |
| `create_col` | Collection object (type varies by backend) |
| `insert` | `None` (void) |
| `delete` | `None` (void) |
| `update` | `None` (void) |
| `delete_col` | `None` (void) |
| `reset` | `None` (void) |

## Concrete Implementations

The codebase includes 22+ vector store implementations, all extending `VectorStoreBase`:
- ChromaDB, Qdrant, Pinecone, PGVector, Milvus, MongoDB, Redis, Weaviate, FAISS, Elasticsearch, OpenSearch, Azure AI Search, Supabase, and more.

## Q4: What data types, models, or structures are used to pass data in and out of vector store methods? How are results normalized across different backends?

# Data Types, Models, and Structures in Vector Store Methods

## Overview

The Mem0 vector store layer uses a consistent pattern for data input/output with some variations across backends. Results are normalized through a two-stage process: backend-specific OutputData models, then higher-level MemoryItem transformation.

## Input Data Types

### Insert Method
All vector store implementations accept the same input signature:
- vectors: List[List[float]] - List of embedding vectors
- payloads: Optional[List[Dict]] - List of metadata dictionaries
- ids: Optional[List[str]] - List of unique identifiers

**Payload Structure** (passed in payloads):
- data: str - The memory text content
- hash: str - Content hash for deduplication
- created_at: str - ISO timestamp
- updated_at: str - ISO timestamp
- user_id, agent_id, run_id: Optional identifiers

### Search Method
- query: str - Query text (for reference)
- vectors: List[float] - Query embedding vector
- limit: int - Number of results (default 5)
- filters: Optional[Dict] - Filter conditions

## Output Data Models

### Stage 1: OutputData (Per-Backend Normalization)

Each vector store implementation defines a local OutputData Pydantic model:

class OutputData(BaseModel):
    id: Optional[str]       # Memory identifier
    score: Optional[float]  # Similarity/distance score
    payload: Optional[Dict] # Metadata dictionary

**Implementations with OutputData**: ChromaDB, Pinecone, Milvus, Elasticsearch, PGVector, FAISS, MongoDB, Cassandra, Azure AI Search, Azure MySQL, Baidu, Redis, Valkey, Databricks, Supabase, Upstash Vector, Weaviate, Vertex AI, OpenSearch, S3 Vectors.

Most backends implement a _parse_output() method to transform native client results to OutputData objects.

### Stage 2: MemoryItem (Higher-Level Normalization)

The memory layer transforms OutputData to MemoryItem:

class MemoryItem(BaseModel):
    id: str                              # Unique identifier
    memory: str                          # Memory text content
    hash: Optional[str] = None          # Content hash
    metadata: Optional[Dict[str, Any]]   # Additional metadata
    score: Optional[float] = None       # Similarity score
    created_at: Optional[str] = None    # Creation timestamp
    updated_at: Optional[str] = None    # Update timestamp

## Backend-Specific Variations

### Inconsistent Return Types
Some backends return raw client objects instead of normalized OutputData:

- Qdrant: Returns hits.points (raw Qdrant ScoredPoint objects)
- Milvus: Returns List[OutputData] via _parse_output()
- ChromaDB: Returns List[OutputData] via _parse_output()
- Pinecone: Returns List[OutputData] converting ScoredVector

### Handling Different Return Formats

The memory layer includes adaptive handling for different return formats in _get_all_from_vector_store(), checking if results are nested lists or flat lists and unwrapping as needed.

## Configuration Models

VectorStoreConfig (in mem0/vector_stores/configs.py):
- provider: str - Provider name (e.g., "qdrant", "chroma")
- config: Optional[Dict] - Provider-specific configuration

Provider-specific configs are located in mem0/configs/vector_stores/ (e.g., QdrantConfig, ChromaDbConfig).

## Summary

1. Input Normalization: Standardized insert(vectors, payloads, ids) signature across all backends
2. Output Normalization: Local OutputData Pydantic model per backend with id, score, payload fields
3. Higher-Level Normalization: Memory layer transforms to MemoryItem with promoted keys and structured metadata
4. Inconsistency: Qdrant and some other backends return raw client objects; memory layer handles both formats
5. Filter Normalization: Each backend implements _create_filter() to translate generic filter dicts to native query syntax

## Q5: How does a user specify which vector store backend to use? What does the configuration look like?

# Vector Store Backend Configuration in Mem0

## How to Specify the Vector Store Backend

Users specify the vector store backend through the `MemoryConfig` class, which is passed to the `Memory` class constructor. The configuration uses a two-level structure:

1. **`provider`**: A string identifying the vector store backend (e.g., `'qdrant'`, `'chroma'`, `'pinecone'`)
2. **`config`**: A dictionary containing provider-specific configuration options

## Configuration Structure

The configuration flows through these classes:
- `MemoryConfig` (in `mem0/configs/base.py`) contains a `vector_store` field
- `VectorStoreConfig` (in `mem0/vector_stores/configs.py`) has `provider` and `config` fields
- Each provider has its own config class in `mem0/configs/vector_stores/<provider>.py`

## Available Providers

24 vector store providers are supported:
- **qdrant** (QdrantConfig) - Default provider
- **chroma** (ChromaDbConfig)
- **pinecone** (PineconeConfig)
- **pgvector** (PGVectorConfig)
- **milvus** (MilvusDBConfig)
- **mongodb** (MongoDBConfig)
- **redis** (RedisDBConfig)
- **weaviate** (WeaviateConfig)
- **faiss** (FAISSConfig)
- **elasticsearch** (ElasticsearchConfig)
- **opensearch** (OpenSearchConfig)
- **supabase** (SupabaseConfig)
- **azure_ai_search** (AzureAISearchConfig)
- **upstash_vector** (UpstashVectorConfig)
- And more...

## Configuration Examples

### Default (Qdrant local)
```python
from mem0 import Memory

memory = Memory()  # Uses Qdrant by default with local storage
```

### ChromaDB (local)
```python
from mem0 import Memory
from mem0.configs.base import MemoryConfig

config = MemoryConfig(
    vector_store={
        "provider": "chroma",
        "config": {
            "collection_name": "my_memories",
            "path": "./chroma_db"
        }
    }
)
memory = Memory(config)
```

### Qdrant (cloud/server)
```python
config = MemoryConfig(
    vector_store={
        "provider": "qdrant",
        "config": {
            "collection_name": "my_collection",
            "url": "https://your-qdrant-instance.com",
            "api_key": "your-api-key"
        }
    }
)
memory = Memory(config)
```

### Pinecone
```python
config = MemoryConfig(
    vector_store={
        "provider": "pinecone",
        "config": {
            "collection_name": "mem0-index",
            "api_key": "your-pinecone-api-key"
        }
    }
)
memory = Memory(config)
```

### PGVector (PostgreSQL)
```python
config = MemoryConfig(
    vector_store={
        "provider": "pgvector",
        "config": {
            "dbname": "memories",
            "collection_name": "mem0",
            "user": "postgres",
            "password": "password",
            "host": "localhost",
            "port": 5432
        }
    }
)
memory = Memory(config)
```

### Redis
```python
config = MemoryConfig(
    vector_store={
        "provider": "redis",
        "config": {
            "collection_name": "memories",
            "redis_url": "redis://localhost:6379"
        }
    }
)
memory = Memory(config)
```

## Validation

Each provider config class includes Pydantic validators that ensure required fields are provided. For example:
- ChromaDB requires either local config (path or host/port) OR cloud config (api_key + tenant)
- Qdrant requires either path (local), host+port (server), or url+api_key (cloud)
- PGVector requires user+password+host+port OR a connection_string

The configuration is validated at initialization time, providing immediate feedback on missing or invalid settings.

## Q6: How does the system validate, resolve, and instantiate a vector store from configuration? Trace the full path from user config to a live backend instance.


# Vector Store Configuration and Instantiation Flow

The system validates, resolves, and instantiates a vector store from configuration through a well-defined multi-stage pipeline:

## Stage 1: Configuration Entry Point (`mem0/configs/base.py`)

The `MemoryConfig` class serves as the top-level configuration container:

```python
class MemoryConfig(BaseModel):
    vector_store: VectorStoreConfig = Field(
        description="Configuration for the vector store",
        default_factory=VectorStoreConfig,
    )
    # ... other fields
```

When a user creates a `Memory` instance, they can provide a `MemoryConfig` with vector store settings.

## Stage 2: Provider Resolution and Config Validation (`mem0/vector_stores/configs.py`)

The `VectorStoreConfig` class handles provider validation and creates provider-specific config objects:

```python
class VectorStoreConfig(BaseModel):
    provider: str = Field(default="qdrant")
    config: Optional[Dict] = Field(default=None)

    _provider_configs: Dict[str, str] = {
        "qdrant": "QdrantConfig",
        "chroma": "ChromaDbConfig",
        "pgvector": "PGVectorConfig",
        # ... 20+ providers
    }

    @model_validator(mode="after")
    def validate_and_create_config(self) -> "VectorStoreConfig":
        # 1. Validate provider is supported
        if provider not in self._provider_configs:
            raise ValueError(f"Unsupported vector store provider: {provider}")

        # 2. Dynamically import provider-specific config module
        module = __import__(
            f"mem0.configs.vector_stores.{provider}",
            fromlist=[self._provider_configs[provider]],
        )
        config_class = getattr(module, self._provider_configs[provider])

        # 3. Instantiate provider-specific config with defaults
        self.config = config_class(**config)
        return self
```

This stage:
- Validates the provider name against the supported list
- Dynamically imports the provider-specific config module
- Instantiates the appropriate config class

## Stage 3: Provider-Specific Config Validation

Each provider has its own config class with additional validation. Example: `QdrantConfig` (`mem0/configs/vector_stores/qdrant.py`):

```python
class QdrantConfig(BaseModel):
    collection_name: str = Field("mem0")
    embedding_model_dims: Optional[int] = Field(1536)
    client: Optional[QdrantClient] = Field(None)
    host: Optional[str] = Field(None)
    port: Optional[int] = Field(None)
    path: Optional[str] = Field("/tmp/qdrant")
    url: Optional[str] = Field(None)
    api_key: Optional[str] = Field(None)
    on_disk: Optional[bool] = Field(False)

    @model_validator(mode="before")
    def check_host_port_or_path(cls, values):
        # Validates connection parameters
        if not path and not (host and port) and not (url and api_key):
            raise ValueError("Either 'host' and 'port' or 'url' and 'api_key' or 'path' must be provided.")
        return values

    @model_validator(mode="before")
    def validate_extra_fields(cls, values):
        # Rejects unknown fields
        extra_fields = input_fields - allowed_fields
        if extra_fields:
            raise ValueError(f"Extra fields not allowed: {', '.join(extra_fields)}")
        return values
```

Similarly, `ChromaDbConfig` validates cloud vs local configuration:

```python
@model_validator(mode="before")
def check_connection_config(cls, values):
    cloud_config = bool(api_key and tenant)
    local_config = bool(path) or bool(host and port)

    if not cloud_config and not local_config:
        raise ValueError("Either cloud config or local config must be provided")
    if cloud_config and local_config:
        raise ValueError("Cannot specify both cloud and local configuration")
    return values
```

## Stage 4: Factory Instantiation (`mem0/utils/factory.py`)

The `VectorStoreFactory` resolves class paths and creates instances:

```python
class VectorStoreFactory:
    provider_to_class = {
        "qdrant": "mem0.vector_stores.qdrant.Qdrant",
        "chroma": "mem0.vector_stores.chroma.ChromaDB",
        "pgvector": "mem0.vector_stores.pgvector.PGVector",
        # ... 20+ providers
    }

    @classmethod
    def create(cls, provider_name, config):
        class_type = cls.provider_to_class.get(provider_name)
        if class_type:
            if not isinstance(config, dict):
                config = config.model_dump()  # Convert Pydantic model to dict
            vector_store_instance = load_class(class_type)
            return vector_store_instance(**config)
        else:
            raise ValueError(f"Unsupported VectorStore provider: {provider_name}")
```

The `load_class` helper dynamically imports and returns the class:
```python
def load_class(class_type):
    module_path, class_name = class_type.rsplit(".", 1)
    module = importlib.import_module(module_path)
    return getattr(module, class_name)
```

## Stage 5: Backend Client Instantiation

The vector store implementation creates the actual backend client. Example: `Qdrant` class (`mem0/vector_stores/qdrant.py`):

```python
class Qdrant(VectorStoreBase):
    def __init__(self, collection_name, embedding_model_dims, client=None,
                 host=None, port=None, path=None, url=None, api_key=None, on_disk=False):
        if client:
            self.client = client  # Use provided client
        else:
            params = {}
            if api_key: params["api_key"] = api_key
            if url: params["url"] = url
            if host and port: params["host"], params["port"] = host, port
            if not params: params["path"] = path

            self.client = QdrantClient(**params)  # Create backend client

        self.create_col(embedding_model_dims, on_disk)  # Initialize collection
```

## Stage 6: Memory Class Integration (`mem0/memory/main.py`)

The `Memory` class ties everything together:

```python
class Memory(MemoryBase):
    def __init__(self, config: MemoryConfig = MemoryConfig()):
        self.config = config

        # Embedder gets vector config for dimension info
        self.embedding_model = EmbedderFactory.create(
            self.config.embedder.provider,
            self.config.embedder.config,
            self.config.vector_store.config,
        )

        # Vector store instantiation via factory
        self.vector_store = VectorStoreFactory.create(
            self.config.vector_store.provider, 
            self.config.vector_store.config
        )

        self.collection_name = self.config.vector_store.config.collection_name
```

## Complete Flow Summary

```
User Config (dict/MemoryConfig)
    ↓
MemoryConfig.vector_store: VectorStoreConfig
    ↓
VectorStoreConfig.validate_and_create_config()
    ├── Validates provider name
    ├── Dynamically imports mem0.configs.vector_stores.{provider}
    └── Instantiates {Provider}Config(**config_dict)
    ↓
Provider-specific config validation
    ├── Field validation (types, defaults)
    ├── @model_validator for connection logic
    └── Extra field rejection
    ↓
VectorStoreFactory.create(provider_name, config)
    ├── Resolves class path from provider_to_class mapping
    ├── load_class() imports the module and gets class
    └── Instantiates with config.model_dump() as kwargs
    ↓
Vector Store Implementation (e.g., Qdrant)
    ├── Receives config values as constructor kwargs
    ├── Creates backend client (QdrantClient, chromadb.Client, etc.)
    └── Initializes collections/indexes
    ↓
Live Backend Instance Ready for Use
```

## Key Design Patterns

1. **Dynamic Import Pattern**: Both config resolution and factory use `importlib` for extensibility
2. **Pydantic Validation**: Multi-layer validation with `@model_validator` for complex rules
3. **Factory Pattern**: `VectorStoreFactory` abstracts instantiation from configuration
4. **Provider-Config Mapping**: Central registries (`_provider_configs`, `provider_to_class`) map names to classes
5. **Default Path Injection**: `VectorStoreConfig` automatically adds `/tmp/{provider}` path if not specified


## Q7: How are vector store backends tested? Is there a shared test structure, or are tests independent per backend?

# Vector Store Backend Testing Structure

## Summary
Vector store backends are tested **independently per backend** with **no shared test structure**. While all implementations share a common interface (`VectorStoreBase`), the tests themselves are completely independent.

## The Common Interface

All vector store implementations inherit from `VectorStoreBase` (defined in `mem0/vector_stores/base.py`), which defines 12 abstract methods:

```python
class VectorStoreBase(ABC):
    @abstractmethod
    def create_col(self, name, vector_size, distance): ...
    @abstractmethod
    def insert(self, vectors, payloads=None, ids=None): ...
    @abstractmethod
    def search(self, query, vectors, limit=5, filters=None): ...
    @abstractmethod
    def delete(self, vector_id): ...
    @abstractmethod
    def update(self, vector_id, vector=None, payload=None): ...
    @abstractmethod
    def get(self, vector_id): ...
    @abstractmethod
    def list_cols(self): ...
    @abstractmethod
    def delete_col(self): ...
    @abstractmethod
    def col_info(self): ...
    @abstractmethod
    def list(self, filters=None, limit=None): ...
    @abstractmethod
    def reset(self): ...
```

## No Shared Test Infrastructure

- **No conftest.py** file exists in the test directory
- **No shared test utilities** or helper modules
- **No base test class** that backends inherit from

## Independent Tests Per Backend

Each of the 22 vector store backends has its own test file in `tests/vector_stores/`:

| Backend | Test File |
|---------|-----------|
| Azure AI Search | test_azure_ai_search.py |
| Azure MySQL | test_azure_mysql.py |
| Baidu | test_baidu.py |
| Cassandra | test_cassandra.py |
| Chroma | test_chroma.py |
| Databricks | test_databricks.py |
| Elasticsearch | test_elasticsearch.py |
| FAISS | test_faiss.py |
| LangChain | test_langchain_vector_store.py |
| Milvus | test_milvus.py |
| MongoDB | test_mongodb.py |
| Neptune Analytics | test_neptune_analytics.py |
| OpenSearch | test_opensearch.py |
| PGVector | test_pgvector.py |
| Pinecone | test_pinecone.py |
| Qdrant | test_qdrant.py |
| S3 Vectors | test_s3_vectors.py |
| Supabase | test_supabase.py |
| Upstash Vector | test_upstash_vector.py |
| Valkey | test_valkey.py |
| Vertex AI | test_vertex_ai_vector_search.py |
| Weaviate | test_weaviate.py |

## Mixed Testing Frameworks

The test files use different testing frameworks:

### unittest.TestCase (3 backends)
- test_qdrant.py
- test_pgvector.py  
- test_elasticsearch.py

These use `unittest.TestCase` with `setUp()` methods and class-based mocking.

### pytest with fixtures (19 backends)
- test_chroma.py, test_pinecone.py, test_milvus.py, test_faiss.py
- test_mongodb.py, test_azure_ai_search.py, test_supabase.py
- And others...

These use `@pytest.fixture` decorators and function-based mocking.

## Common Test Patterns (Independently Implemented)

While tests are independent, they follow similar patterns testing the common interface:

- `test_insert` / `test_insert_vectors`
- `test_search` / `test_search_vectors`
- `test_delete` / `test_delete_vector`
- `test_update` / `test_update_vector`
- `test_get` / `test_get_vector`
- `test_list` / `test_list_vectors`
- `test_create_col` / `test_create_index`
- `test_delete_col`

Each test file defines its own fixtures to mock the specific client library for that backend (e.g., `mock_chromadb_client`, `mock_pinecone_client`, `mock_milvus_client`).

## Conclusion

The testing approach prioritizes **backend independence** over code reuse. Each backend has complete control over its testing strategy, fixtures, and mocking approach. This makes sense given that each vector store has different client libraries, connection methods, and specific features that need to be tested independently.

## Q8: What mocking or fixture patterns are used to isolate tests from real database connections?


# Mocking and Fixture Patterns for Database Isolation in Mem0 Tests

The Mem0 test suite uses **multiple complementary patterns** to isolate tests from real database connections. These patterns range from full mocking strategies to lightweight isolation techniques.

## Pattern 1: unittest.TestCase + MagicMock (Classic Approach)

Used in 11 test files, this pattern leverages Python's built-in unittest framework with `MagicMock` for comprehensive client mocking.

**Example from `tests/vector_stores/test_qdrant.py`:**
```python
from unittest.mock import MagicMock
from qdrant_client import QdrantClient

class TestQdrant(unittest.TestCase):
    def setUp(self):
        self.client_mock = MagicMock(spec=QdrantClient)  # Spec ensures interface compliance
        self.qdrant = Qdrant(
            collection_name="test_collection",
            embedding_model_dims=128,
            client=self.client_mock,  # Injected mock
            path="test_path",
            on_disk=True,
        )
```

**Key benefits:**
- `spec=QdrantClient` ensures mocks only allow real client methods
- Mock client is injected via constructor, enabling full isolation
- No real Qdrant server needed

---

## Pattern 2: pytest Fixtures + patch() Context Manager

Used in 39 test files with `@pytest.fixture`, this pattern uses context managers for scoped mocking.

**Example from `tests/vector_stores/test_chroma.py`:**
```python
from unittest.mock import Mock, patch
import pytest

@pytest.fixture
def mock_chromadb_client():
    with patch("chromadb.Client") as mock_client:
        yield mock_client  # Mock active only within test scope

@pytest.fixture
def chromadb_instance(mock_chromadb_client):
    mock_collection = Mock()
    mock_chromadb_client.return_value.get_or_create_collection.return_value = mock_collection
    return ChromaDB(collection_name="test_collection", client=mock_chromadb_client.return_value)
```

---

## Pattern 3: pytest-mock's `mocker` Fixture

The `mocker` fixture (from pytest-mock plugin) provides a cleaner API for patching.

**Example from `tests/memory/test_main.py`:**
```python
def _setup_mocks(mocker):
    # Mock the embedder factory
    mock_embedder = mocker.MagicMock()
    mock_embedder.return_value.embed.return_value = [0.1, 0.2, 0.3]
    mocker.patch("mem0.utils.factory.EmbedderFactory.create", mock_embedder)

    # Mock the vector store factory
    mock_vector_store = mocker.MagicMock()
    mocker.patch("mem0.utils.factory.VectorStoreFactory.create", mock_vector_store)

    # Mock SQLiteManager to avoid real DB
    mocker.patch("mem0.memory.storage.SQLiteManager", mocker.MagicMock())

    return mock_llm, mock_vector_store

class TestAddToVectorStoreErrors:
    @pytest.fixture
    def mock_memory(self, mocker):
        mock_llm, _ = _setup_mocks(mocker)
        memory = Memory()
        memory.config = mocker.MagicMock()
        return memory
```

---

## Pattern 4: Decorator-Based Patching on Fixtures

Combines `@patch` decorators with pytest fixtures for module-level interception.

**Example from `tests/vector_stores/test_mongodb.py`:**
```python
@pytest.fixture
@patch("mem0.vector_stores.mongodb.MongoClient")  # Intercept at module level
def mongo_vector_fixture(mock_mongo_client):
    mock_client = mock_mongo_client.return_value
    mock_db = mock_client["test_db"]
    mock_collection = mock_db["test_collection"]
    mock_collection.list_search_indexes.return_value = []

    mongo_vector = MongoDB(
        db_name="test_db",
        collection_name="test_collection",
        embedding_model_dims=1536,
        mongo_uri="mongodb://username:password@localhost:27017",
    )
    return mongo_vector, mock_collection, mock_db
```

---

## Pattern 5: tempfile-Based File Isolation

For file-based vector stores, temporary directories provide real filesystem isolation without persistence.

**Example from `tests/vector_stores/test_faiss.py`:**
```python
@pytest.fixture
def faiss_instance(mock_faiss_index):
    with tempfile.TemporaryDirectory() as temp_dir:  # Auto-cleanup
        with patch("faiss.IndexFlatL2", return_value=mock_faiss_index):
            with patch("faiss.write_index"):
                faiss_store = FAISS(
                    collection_name="test_collection",
                    path=os.path.join(temp_dir, "test_faiss"),
                    distance_strategy="euclidean",
                )
                yield faiss_store  # Directory cleaned up after test
```

**Example from `tests/memory/test_storage.py`:**
```python
@pytest.fixture
def temp_db_path(self):
    temp_db = tempfile.NamedTemporaryFile(delete=False, suffix=".db")
    temp_db.close()
    yield temp_db.name
    if os.path.exists(temp_db.name):
        os.unlink(temp_db.name)  # Cleanup in teardown
```

---

## Pattern 6: SQLite :memory: In-Memory Databases

Uses SQLite's built-in in-memory mode for real SQL operations without file persistence.

**Example from `tests/memory/test_storage.py`:**
```python
@pytest.fixture
def memory_manager(self):
    manager = SQLiteManager(":memory:")  # In-memory DB, no file created
    yield manager
    if manager.connection:
        manager.close()  # Connection cleanup
```

---

## Pattern 7: Environment Variable Isolation

For tests that need to control environment-based configuration, uses `setUpClass`/`tearDownClass` lifecycle methods.

**Example from `tests/vector_stores/test_elasticsearch.py`:**
```python
class TestElasticsearchDB(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        dotenv.load_dotenv()
        # Save original environment
        cls.original_env = {
            "ES_URL": os.getenv("ES_URL", "http://localhost:9200"),
            "ES_USERNAME": os.getenv("ES_USERNAME", "test_user"),
            "ES_PASSWORD": os.getenv("ES_PASSWORD", "test_password"),
        }
        # Set test environment
        os.environ["ES_URL"] = "http://localhost"
        os.environ["ES_USERNAME"] = "test_user"
        os.environ["ES_PASSWORD"] = "test_password"

    @classmethod
    def tearDownClass(cls):
        # Restore original environment
        for key, value in cls.original_env.items():
            if value is not None:
                os.environ[key] = value
            else:
                os.environ.pop(key, None)
```

---

## Pattern 8: Constructor Dependency Injection

The most common pattern - passing mock clients directly via constructor parameters, enabling pure unit testing.

**Example from `tests/vector_stores/test_pinecone.py`:**
```python
@pytest.fixture
def mock_pinecone_client():
    client = MagicMock()
    client.Index.return_value = MagicMock()
    client.list_indexes.return_value.names.return_value = []
    return client

@pytest.fixture
def pinecone_db(mock_pinecone_client):
    return PineconeDB(
        collection_name="test_index",
        embedding_model_dims=128,
        client=mock_pinecone_client,  # Direct injection
        api_key="fake_api_key",
        # ... other params
    )
```

---

## Pattern 9: Mocking Third-Party Client Factories

For services like AWS S3, intercept at the factory level.

**Example from `tests/vector_stores/test_s3_vectors.py`:**
```python
@pytest.fixture
def mock_boto_client(mocker):
    mock_client = mocker.MagicMock()
    mocker.patch("boto3.client", return_value=mock_client)  # Intercept boto3.client()
    return mock_client
```

---

## Summary Statistics

| Pattern | Files Using | Complexity |
|---------|-------------|------------|
| pytest fixtures | 39 | Medium |
| MagicMock | 18 | Low |
| unittest.TestCase | 11 | Medium |
| @patch decorators | 5 | Medium |
| mocker fixture (pytest-mock) | 3 | Low |
| tempfile isolation | 2 | Low |
| SQLite :memory: | 2 | Very Low |

**Key Takeaways:**
1. **No testcontainers or Docker** - All isolation is achieved through mocking and in-process techniques
2. **Constructor injection is preferred** - Most vector stores accept a `client` parameter for mock injection
3. **Factory patching for complex dependencies** - EmbedderFactory, VectorStoreFactory, LlmFactory are patched at the module level
4. **SQLite :memory: for storage tests** - Real SQL operations without persistence overhead


## Q9: Based on everything we've discussed, produce a step-by-step integration guide for adding a new vector store backend to this project. Cover every file to create, every file to modify, every registration point, config class, and test requirement. The guide should be self-contained.  Include a checklist at the end.

# Step-by-Step Integration Guide: Adding a New Vector Store Backend to Mem0

This guide provides complete instructions for adding a new vector store backend to the Mem0 project. Follow each step carefully to ensure proper integration.

---

## Overview

To add a new vector store backend, you need to:

1. Create a configuration class
2. Create the implementation class
3. Register in two locations (config and factory)
4. Add dependencies
5. Create tests

---

## Step 1: Create the Configuration Class

**File to create:** `mem0/configs/vector_stores/<provider_name>.py`

### Template:

```python
from typing import Any, ClassVar, Dict, Optional

from pydantic import BaseModel, ConfigDict, Field, model_validator


class <ProviderName>Config(BaseModel):
    """
    Configuration class for <ProviderName> vector store.
    """

    # Import the client class as a ClassVar for type hints
    # (Only if the client library is always available)
    # from <client_library> import <ClientClass>
    # <ClientClass>: ClassVar[type] = <ClientClass>

    # Required/Optional configuration fields
    collection_name: str = Field("mem0", description="Name of the collection")
    embedding_model_dims: Optional[int] = Field(1536, description="Dimensions of the embedding model")

    # Connection parameters - adapt based on your provider
    host: Optional[str] = Field(None, description="Host address for the server")
    port: Optional[int] = Field(None, description="Port for the server")
    path: Optional[str] = Field(None, description="Path for local database")
    url: Optional[str] = Field(None, description="Full URL for the server")
    api_key: Optional[str] = Field(None, description="API key for authentication")

    # Optional: Existing client injection
    client: Optional[Any] = Field(None, description="Existing client instance")

    @model_validator(mode="before")
    @classmethod
    def check_connection_params(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that required connection parameters are provided."""
        # Add your validation logic here
        # Example: ensure either path OR (host and port) OR (url and api_key) is provided
        host = values.get("host")
        port = values.get("port")
        path = values.get("path")
        url = values.get("url")
        api_key = values.get("api_key")
        client = values.get("client")

        # If client is provided, skip validation
        if client is not None:
            return values

        # Add your validation logic
        # if not path and not (host and port) and not (url and api_key):
        #     raise ValueError("Either path, host+port, or url+api_key must be provided")

        return values

    @model_validator(mode="before")
    @classmethod
    def validate_extra_fields(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """Reject extra fields not in the model."""
        allowed_fields = set(cls.model_fields.keys())
        input_fields = set(values.keys())
        extra_fields = input_fields - allowed_fields
        if extra_fields:
            raise ValueError(
                f"Extra fields not allowed: {', '.join(extra_fields)}. "
                f"Please input only the following fields: {', '.join(allowed_fields)}"
            )
        return values

    model_config = ConfigDict(arbitrary_types_allowed=True)
```

### Example (simplified Qdrant-style):

```python
from typing import Any, ClassVar, Dict, Optional

from pydantic import BaseModel, ConfigDict, Field, model_validator


class MyVectorStoreConfig(BaseModel):
    from my_vector_client import MyVectorClient

    MyVectorClient: ClassVar[type] = MyVectorClient

    collection_name: str = Field("mem0", description="Name of the collection")
    embedding_model_dims: Optional[int] = Field(1536, description="Dimensions of the embedding model")
    client: Optional[MyVectorClient] = Field(None, description="Existing client instance")
    host: Optional[str] = Field(None, description="Host address")
    port: Optional[int] = Field(None, description="Port number")
    path: Optional[str] = Field("/tmp/my_vector_store", description="Local database path")
    api_key: Optional[str] = Field(None, description="API key")

    @model_validator(mode="before")
    @classmethod
    def check_connection(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        if values.get("client") is not None:
            return values
        if not values.get("path") and not (values.get("host") and values.get("port")):
            if not values.get("api_key"):
                values["path"] = "/tmp/my_vector_store"
        return values

    model_config = ConfigDict(arbitrary_types_allowed=True)
```

---

## Step 2: Create the Implementation Class

**File to create:** `mem0/vector_stores/<provider_name>.py`

### Required Abstract Methods (all 11 must be implemented):

| Method | Signature | Description |
|--------|-----------|-------------|
| `create_col` | `create_col(name, vector_size, distance)` | Create a new collection |
| `insert` | `insert(vectors, payloads=None, ids=None)` | Insert vectors into collection |
| `search` | `search(query, vectors, limit=5, filters=None)` | Search for similar vectors |
| `delete` | `delete(vector_id)` | Delete a vector by ID |
| `update` | `update(vector_id, vector=None, payload=None)` | Update vector and/or payload |
| `get` | `get(vector_id)` | Retrieve a vector by ID |
| `list_cols` | `list_cols()` | List all collections |
| `delete_col` | `delete_col()` | Delete the collection |
| `col_info` | `col_info()` | Get collection information |
| `list` | `list(filters=None, limit=None)` | List all memories |
| `reset` | `reset()` | Delete and recreate collection |

### Template:

```python
import logging
from typing import Dict, List, Optional

from pydantic import BaseModel

try:
    from my_vector_client import MyVectorClient
except ImportError:
    raise ImportError(
        "The 'my_vector_client' library is required. "
        "Please install it using 'pip install my-vector-client'."
    )

from mem0.vector_stores.base import VectorStoreBase

logger = logging.getLogger(__name__)


class OutputData(BaseModel):
    """Standard output format for search results."""
    id: Optional[str]
    score: Optional[float]
    payload: Optional[Dict]


class MyVectorStore(VectorStoreBase):
    """
    MyVectorStore implementation for Mem0.
    """

    def __init__(
        self,
        collection_name: str,
        embedding_model_dims: int,
        client: Optional[MyVectorClient] = None,
        host: Optional[str] = None,
        port: Optional[int] = None,
        path: Optional[str] = None,
        api_key: Optional[str] = None,
        **kwargs,
    ):
        """
        Initialize the vector store.

        Args:
            collection_name: Name of the collection
            embedding_model_dims: Dimensions of the embedding model
            client: Existing client instance (optional)
            host: Server host address
            port: Server port
            path: Local database path
            api_key: API key for authentication
        """
        # Initialize client
        if client:
            self.client = client
        elif host and port:
            self.client = MyVectorClient(host=host, port=port, api_key=api_key)
        elif path:
            self.client = MyVectorClient(path=path)
        else:
            self.client = MyVectorClient()  # Default initialization

        self.collection_name = collection_name
        self.embedding_model_dims = embedding_model_dims

        # Create collection on init
        self.create_col(embedding_model_dims)

    def create_col(self, vector_size: int, distance: str = "cosine"):
        """Create a new collection."""
        # Check if collection exists
        existing = self.list_cols()
        for col in existing:
            if col.name == self.collection_name:
                logger.debug(f"Collection {self.collection_name} already exists")
                return

        # Create the collection
        self.client.create_collection(
            name=self.collection_name,
            vector_size=vector_size,
            distance=distance,
        )
        logger.info(f"Created collection {self.collection_name}")

    def insert(
        self,
        vectors: List[list],
        payloads: Optional[List[Dict]] = None,
        ids: Optional[List[str]] = None,
    ):
        """Insert vectors into the collection."""
        logger.info(f"Inserting {len(vectors)} vectors into {self.collection_name}")

        points = []
        for i, vector in enumerate(vectors):
            point = {
                "id": ids[i] if ids else str(i),
                "vector": vector,
                "payload": payloads[i] if payloads else {},
            }
            points.append(point)

        self.client.upsert(
            collection_name=self.collection_name,
            points=points,
        )

    def search(
        self,
        query: str,
        vectors: List[list],
        limit: int = 5,
        filters: Optional[Dict] = None,
    ):
        """Search for similar vectors."""
        # Convert filters to provider-specific format if needed
        query_filter = self._build_filter(filters) if filters else None

        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=vectors[0] if isinstance(vectors[0], list) else vectors,
            limit=limit,
            filter=query_filter,
        )

        return [
            OutputData(
                id=str(result.id),
                score=result.score,
                payload=result.payload,
            )
            for result in results
        ]

    def _build_filter(self, filters: Dict):
        """Convert Mem0 filter dict to provider-specific filter format."""
        # Implement based on your provider's filter syntax
        # Common filter fields: user_id, agent_id, run_id, actor_id
        conditions = []
        for key, value in filters.items():
            conditions.append({"key": key, "value": value})
        return {"must": conditions}

    def delete(self, vector_id: str):
        """Delete a vector by ID."""
        self.client.delete(
            collection_name=self.collection_name,
            ids=[vector_id],
        )

    def update(
        self,
        vector_id: str,
        vector: Optional[List[float]] = None,
        payload: Optional[Dict] = None,
    ):
        """Update a vector and/or its payload."""
        update_data = {"id": vector_id}
        if vector is not None:
            update_data["vector"] = vector
        if payload is not None:
            update_data["payload"] = payload

        self.client.upsert(
            collection_name=self.collection_name,
            points=[update_data],
        )

    def get(self, vector_id: str) -> Optional[OutputData]:
        """Retrieve a vector by ID."""
        results = self.client.retrieve(
            collection_name=self.collection_name,
            ids=[vector_id],
        )

        if results:
            return OutputData(
                id=str(results[0].id),
                score=None,
                payload=results[0].payload,
            )
        return None

    def list_cols(self):
        """List all collections."""
        return self.client.get_collections()

    def delete_col(self):
        """Delete the collection."""
        self.client.delete_collection(self.collection_name)
        logger.info(f"Deleted collection {self.collection_name}")

    def col_info(self):
        """Get information about the collection."""
        return self.client.get_collection(self.collection_name)

    def list(self, filters: Optional[Dict] = None, limit: Optional[int] = None):
        """List all memories in the collection."""
        # Use scroll/scan operation if available
        query_filter = self._build_filter(filters) if filters else None

        results = self.client.scroll(
            collection_name=self.collection_name,
            filter=query_filter,
            limit=limit or 100,
        )

        return [
            OutputData(
                id=str(result.id),
                score=None,
                payload=result.payload,
            )
            for result in results
        ]

    def reset(self):
        """Reset by deleting and recreating the collection."""
        self.delete_col()
        self.create_col(self.embedding_model_dims)
```

---

## Step 3: Register in VectorStoreConfig

**File to modify:** `mem0/vector_stores/configs.py`

### Location: `VectorStoreConfig._provider_configs` dictionary

Add your provider to the `_provider_configs` dictionary:

```python
_provider_configs: Dict[str, str] = {
    # ... existing entries ...
    "my_vector_store": "MyVectorStoreConfig",  # <-- ADD THIS LINE
}
```

---

## Step 4: Register in VectorStoreFactory

**File to modify:** `mem0/utils/factory.py`

### Location: `VectorStoreFactory.provider_to_class` dictionary

Add your provider to the `provider_to_class` dictionary:

```python
class VectorStoreFactory:
    provider_to_class = {
        # ... existing entries ...
        "my_vector_store": "mem0.vector_stores.my_vector_store.MyVectorStore",  # <-- ADD THIS LINE
    }
```

---

## Step 5: Add Dependencies

**File to modify:** `pyproject.toml`

### Location: `[project.optional-dependencies]` → `vector_stores` array

Add your provider's client library:

```toml
[project.optional-dependencies]
vector_stores = [
    # ... existing entries ...
    "my-vector-client>=1.0.0",  # <-- ADD THIS LINE
]
```

---

## Step 6: Create Tests

**File to create:** `tests/vector_stores/test_<provider_name>.py`

### Template:

```python
import unittest
import uuid
from unittest.mock import MagicMock, patch

from my_vector_client import MyVectorClient

from mem0.vector_stores.my_vector_store import MyVectorStore


class TestMyVectorStore(unittest.TestCase):
    def setUp(self):
        """Set up test fixtures."""
        self.client_mock = MagicMock(spec=MyVectorClient)
        self.vector_store = MyVectorStore(
            collection_name="test_collection",
            embedding_model_dims=128,
            client=self.client_mock,
        )

    def test_create_col(self):
        """Test collection creation."""
        self.client_mock.get_collections.return_value = []

        self.vector_store.create_col(vector_size=128)

        self.client_mock.create_collection.assert_called_once()

    def test_insert(self):
        """Test vector insertion."""
        vectors = [[0.1, 0.2], [0.3, 0.4]]
        payloads = [{"key": "value1"}, {"key": "value2"}]
        ids = [str(uuid.uuid4()), str(uuid.uuid4())]

        self.vector_store.insert(vectors=vectors, payloads=payloads, ids=ids)

        self.client_mock.upsert.assert_called_once()
        call_args = self.client_mock.upsert.call_args
        points = call_args[1]["points"]

        self.assertEqual(len(points), 2)
        self.assertEqual(points[0]["payload"], payloads[0])

    def test_search(self):
        """Test vector search."""
        vectors = [[0.1, 0.2]]
        mock_result = MagicMock(
            id=str(uuid.uuid4()),
            score=0.95,
            payload={"key": "value"}
        )
        self.client_mock.search.return_value = [mock_result]

        results = self.vector_store.search(query="", vectors=vectors, limit=1)

        self.client_mock.search.assert_called_once()
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].payload, {"key": "value"})
        self.assertEqual(results[0].score, 0.95)

    def test_search_with_filters(self):
        """Test search with filters."""
        vectors = [[0.1, 0.2]]
        mock_result = MagicMock(
            id=str(uuid.uuid4()),
            score=0.95,
            payload={"user_id": "alice", "agent_id": "agent1"}
        )
        self.client_mock.search.return_value = [mock_result]

        filters = {"user_id": "alice", "agent_id": "agent1"}
        results = self.vector_store.search(query="", vectors=vectors, limit=1, filters=filters)

        self.client_mock.search.assert_called_once()
        call_args = self.client_mock.search.call_args[1]
        self.assertIsNotNone(call_args["filter"])

        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].payload["user_id"], "alice")

    def test_delete(self):
        """Test vector deletion."""
        vector_id = str(uuid.uuid4())

        self.vector_store.delete(vector_id)

        self.client_mock.delete.assert_called_once()

    def test_update(self):
        """Test vector update."""
        vector_id = str(uuid.uuid4())
        new_vector = [0.5, 0.6]
        new_payload = {"updated": True}

        self.vector_store.update(vector_id, vector=new_vector, payload=new_payload)

        self.client_mock.upsert.assert_called_once()

    def test_get(self):
        """Test vector retrieval."""
        vector_id = str(uuid.uuid4())
        mock_result = MagicMock(
            id=vector_id,
            payload={"key": "value"}
        )
        self.client_mock.retrieve.return_value = [mock_result]

        result = self.vector_store.get(vector_id)

        self.client_mock.retrieve.assert_called_once()
        self.assertEqual(result.id, vector_id)
        self.assertEqual(result.payload, {"key": "value"})

    def test_get_nonexistent(self):
        """Test retrieval of non-existent vector."""
        self.client_mock.retrieve.return_value = []

        result = self.vector_store.get("nonexistent_id")

        self.assertIsNone(result)

    def test_list_cols(self):
        """Test listing collections."""
        mock_cols = [MagicMock(name="col1"), MagicMock(name="col2")]
        self.client_mock.get_collections.return_value = mock_cols

        result = self.vector_store.list_cols()

        self.assertEqual(len(result), 2)

    def test_delete_col(self):
        """Test collection deletion."""
        self.vector_store.delete_col()

        self.client_mock.delete_collection.assert_called_once_with("test_collection")

    def test_col_info(self):
        """Test collection info retrieval."""
        mock_info = MagicMock(vectors_count=100)
        self.client_mock.get_collection.return_value = mock_info

        result = self.vector_store.col_info()

        self.client_mock.get_collection.assert_called_once()

    def test_list(self):
        """Test listing vectors."""
        mock_results = [
            MagicMock(id=str(uuid.uuid4()), payload={"key": "value1"}),
            MagicMock(id=str(uuid.uuid4()), payload={"key": "value2"}),
        ]
        self.client_mock.scroll.return_value = mock_results

        results = self.vector_store.list(limit=10)

        self.assertEqual(len(results), 2)

    def test_list_with_filters(self):
        """Test listing with filters."""
        mock_result = MagicMock(
            id=str(uuid.uuid4()),
            payload={"user_id": "alice"}
        )
        self.client_mock.scroll.return_value = [mock_result]

        results = self.vector_store.list(filters={"user_id": "alice"})

        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].payload["user_id"], "alice")

    def test_reset(self):
        """Test collection reset."""
        self.vector_store.reset()

        self.client_mock.delete_collection.assert_called_once()
        self.client_mock.create_collection.assert_called()


if __name__ == "__main__":
    unittest.main()
```

---

## Step 7: Verify Integration

### Manual Verification:

```python
# Test the integration manually
from mem0 import Memory

m = Memory()

# Test with your new vector store
config = {
    "vector_store": {
        "provider": "my_vector_store",
        "config": {
            "collection_name": "test",
            "host": "localhost",
            "port": 8080,
        }
    }
}

# This should work without errors
m.add("This is a test memory", user_id="test_user")
results = m.search("test", user_id="test_user")
print(results)
```

---

## Integration Checklist

### Files to Create:
- [ ] `mem0/configs/vector_stores/<provider_name>.py` - Configuration class
- [ ] `mem0/vector_stores/<provider_name>.py` - Implementation class
- [ ] `tests/vector_stores/test_<provider_name>.py` - Unit tests

### Files to Modify:
- [ ] `mem0/vector_stores/configs.py` - Add to `_provider_configs` dict
- [ ] `mem0/utils/factory.py` - Add to `VectorStoreFactory.provider_to_class` dict
- [ ] `pyproject.toml` - Add dependency to `vector_stores` extras

### Implementation Checklist:
- [ ] Config class inherits from `pydantic.BaseModel`
- [ ] Config class has all required fields (collection_name, embedding_model_dims, etc.)
- [ ] Config class has model validators for connection parameters
- [ ] Config class has `model_config = ConfigDict(arbitrary_types_allowed=True)`
- [ ] Implementation class inherits from `VectorStoreBase`
- [ ] Implementation class has `try/except` import for client library
- [ ] All 11 abstract methods are implemented:
  - [ ] `create_col(name, vector_size, distance)`
  - [ ] `insert(vectors, payloads=None, ids=None)`
  - [ ] `search(query, vectors, limit=5, filters=None)`
  - [ ] `delete(vector_id)`
  - [ ] `update(vector_id, vector=None, payload=None)`
  - [ ] `get(vector_id)`
  - [ ] `list_cols()`
  - [ ] `delete_col()`
  - [ ] `col_info()`
  - [ ] `list(filters=None, limit=None)`
  - [ ] `reset()`
- [ ] Search method supports filtering (user_id, agent_id, run_id, actor_id)
- [ ] Output format uses `OutputData` model with id, score, payload

### Test Checklist:
- [ ] Tests use `unittest.TestCase`
- [ ] Tests mock the client library
- [ ] Tests cover all 11 methods
- [ ] Tests cover filter scenarios
- [ ] Tests pass: `pytest tests/vector_stores/test_<provider_name>.py`

### Registration Checklist:
- [ ] Added to `VectorStoreConfig._provider_configs` with correct config class name
- [ ] Added to `VectorStoreFactory.provider_to_class` with correct module path
- [ ] Dependency added to `pyproject.toml` under `vector_stores`

---

## Common Pitfalls

1. **Missing method implementation**: All 11 abstract methods must be implemented
2. **Wrong config class name**: Must match exactly in `_provider_configs`
3. **Wrong module path**: Must match actual file location in `provider_to_class`
4. **Missing `arbitrary_types_allowed`**: Required for client types in config
5. **Filter format mismatch**: Ensure filters work with user_id, agent_id, run_id, actor_id
6. **Import error handling**: Use try/except for optional client library imports
7. **Output format inconsistency**: Return `OutputData` objects from search/get/list methods

---

## Summary

To add a new vector store backend:

1. **Create config class** → `mem0/configs/vector_stores/<provider>.py`
2. **Create implementation** → `mem0/vector_stores/<provider>.py`
3. **Register config** → `mem0/vector_stores/configs.py` (`_provider_configs`)
4. **Register implementation** → `mem0/utils/factory.py` (`VectorStoreFactory.provider_to_class`)
5. **Add dependency** → `pyproject.toml` (`vector_stores` extras)
6. **Create tests** → `tests/vector_stores/test_<provider>.py`

The integration is complete when users can use your vector store with:

```python
from mem0 import Memory

m = Memory(config={
    "vector_store": {
        "provider": "my_vector_store",
        "config": {...}
    }
})
```

